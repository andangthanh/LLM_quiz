{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dataclasses import dataclass\n",
    "from dataclasses import asdict\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import time\n",
    "from os import listdir\n",
    "from urllib3.util import Retry\n",
    "from requests import Session\n",
    "from requests.adapters import HTTPAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScrapedData:\n",
    "    url: str\n",
    "    scrape_datetime: datetime\n",
    "    paragraphs: list[int]\n",
    "    total_words: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScrapedJinaAiData:\n",
    "    url: str\n",
    "    scrape_datetime: datetime\n",
    "    content: str\n",
    "    total_words: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        scrape_datetime = datetime.now()\n",
    "        paragraphs = [paragraph.get_text() for paragraph in soup.find_all('p')]\n",
    "        total_words = sum(len(paragraph.split()) for paragraph in paragraphs)\n",
    "\n",
    "        scraped_data = ScrapedData(\n",
    "            url=url,\n",
    "            scrape_datetime=scrape_datetime,\n",
    "            paragraphs=paragraphs,\n",
    "            total_words=total_words\n",
    "        )\n",
    "\n",
    "        return scraped_data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while fetching the URL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_content_jina_ai(url, session): #TODO add time.sleep(2), maximum of 20 RPM\n",
    "    try:\n",
    "        response = session.get(\"http://r.jina.ai/\" + url)\n",
    "        response.raise_for_status()\n",
    "        scrape_datetime = datetime.now()\n",
    "        total_words = len(response.text.split())\n",
    "        scraped_data = ScrapedJinaAiData(\n",
    "            url=\"http://\" + url,\n",
    "            scrape_datetime=scrape_datetime,\n",
    "            content=response.text,\n",
    "            total_words=total_words\n",
    "        )\n",
    "        return scraped_data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while fetching the URL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_content_jina_ai_API_key(url): #TODO \n",
    "    scraped_data = None\n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wiki_Crawler:\n",
    "    def __init__(self):\n",
    "        self.scrape_method = None\n",
    "\n",
    "    def scrape(self, url, session):\n",
    "        if self.scrape_method is None:\n",
    "            print(\"no scrape method set!\")\n",
    "            return None \n",
    "        return self.scrape_method(url, session)\n",
    "    \n",
    "    def set_scrape_method(self, scrape_method):\n",
    "        self.scrape_method = scrape_method\n",
    "\n",
    "    def create_filename(self, url):\n",
    "        filename = urlparse(url).path.split('/')[-1]\n",
    "        if not filename:\n",
    "            filename = \"home\"\n",
    "        else:\n",
    "            filename = re.sub('[^A-Za-z0-9]+', '', filename)       \n",
    "        return filename\n",
    "\n",
    "    def wiki_crawler(self, base_url, max_sites, save_folder):\n",
    "        visited = set()\n",
    "        visited.add(base_url)\n",
    "        queue = [base_url]\n",
    "        counter = 0\n",
    "        already_saved_files = listdir(save_folder)\n",
    "        s = Session()\n",
    "        retries = Retry(\n",
    "            total=3,\n",
    "            backoff_factor=1.5,\n",
    "            status_forcelist=[422 ,502, 503, 504],\n",
    "        )\n",
    "        s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "        while queue and counter < max_sites:\n",
    "            counter += 1\n",
    "            url = queue.pop(0)\n",
    "\n",
    "            try:\n",
    "                response = s.get(url)\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                for link in soup.find_all('a', href=True):\n",
    "                    href = link.get('href')\n",
    "                    absolute_url = urljoin(base_url, href)\n",
    "                    \n",
    "                    if urlparse(absolute_url).hostname == urlparse(base_url).hostname and absolute_url not in visited:\n",
    "                        visited.add(absolute_url)\n",
    "                        queue.append(absolute_url)\n",
    "                \n",
    "                filename = self.create_filename(url) + \".json\"\n",
    "                save_path = save_folder / filename\n",
    "\n",
    "                if filename in already_saved_files:\n",
    "                    continue\n",
    "\n",
    "                scraped_data = self.scrape(url, s)\n",
    "                with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(asdict(scraped_data), f, ensure_ascii=False, indent=4, default=str) \n",
    "\n",
    "                print(f\"Scraped: {url}\")\n",
    "                time.sleep(2)\n",
    "                        \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error while fetching {url}: {e}\")\n",
    "\n",
    "        return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = Path().resolve().parent / \"data/pokemon_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Wiki_Crawler()\n",
    "crawler.set_scrape_method(scrape_content_jina_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = crawler.wiki_crawler(\"https://www.pokewiki.de\",2000,save_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
